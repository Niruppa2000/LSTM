{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb5e98ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 184s 580ms/step - loss: 3.1643 - val_loss: 2.8641\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 205s 657ms/step - loss: 2.7333 - val_loss: 2.6998\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 213s 681ms/step - loss: 2.5834 - val_loss: 2.6224\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 206s 657ms/step - loss: 2.4861 - val_loss: 2.5761\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 209s 669ms/step - loss: 2.4064 - val_loss: 2.5285\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 211s 673ms/step - loss: 2.3340 - val_loss: 2.5015\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 199s 636ms/step - loss: 2.2662 - val_loss: 2.4789\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 213s 681ms/step - loss: 2.2021 - val_loss: 2.4655\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 210s 672ms/step - loss: 2.1421 - val_loss: 2.4582\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 210s 672ms/step - loss: 2.0843 - val_loss: 2.4545\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 212s 677ms/step - loss: 2.0267 - val_loss: 2.4573\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 197s 629ms/step - loss: 1.9727 - val_loss: 2.4650\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 213s 679ms/step - loss: 1.9205 - val_loss: 2.4731\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 204s 650ms/step - loss: 1.8716 - val_loss: 2.4879\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 212s 678ms/step - loss: 1.8238 - val_loss: 2.5007\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 214s 684ms/step - loss: 1.7774 - val_loss: 2.5297\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 204s 651ms/step - loss: 1.7326 - val_loss: 2.5449\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 210s 671ms/step - loss: 1.6882 - val_loss: 2.5553\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 211s 674ms/step - loss: 1.6470 - val_loss: 2.5810\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 213s 681ms/step - loss: 1.6043 - val_loss: 2.6049\n",
      "English: And\n",
      "Hindi: और\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, string\n",
    "from string import digits\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load and preprocess dataset\n",
    "lines = pd.read_csv(\"Hindi_English_Truncated_Corpus.csv\", encoding='utf-8')\n",
    "lines = lines[lines['source'] == 'ted'][['english_sentence', 'hindi_sentence']].dropna().drop_duplicates()\n",
    "lines = lines.sample(n=25000, random_state=42)\n",
    "\n",
    "def clean_text(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    text = ''.join(ch for ch in text if ch not in exclude)\n",
    "    text = text.translate(str.maketrans('', '', digits))\n",
    "    return text.strip().lower()\n",
    "\n",
    "lines['english_sentence'] = lines['english_sentence'].apply(clean_text)\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(clean_text)\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: 'start_ ' + x + ' _end')\n",
    "\n",
    "# Tokenization\n",
    "eng_tokenizer = Tokenizer()\n",
    "eng_tokenizer.fit_on_texts(lines['english_sentence'])\n",
    "eng_seq = eng_tokenizer.texts_to_sequences(lines['english_sentence'])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "\n",
    "hin_tokenizer = Tokenizer(filters='')\n",
    "hin_tokenizer.fit_on_texts(lines['hindi_sentence'])\n",
    "hin_seq = hin_tokenizer.texts_to_sequences(lines['hindi_sentence'])\n",
    "hin_vocab_size = len(hin_tokenizer.word_index) + 1\n",
    "\n",
    "# Sequence Padding\n",
    "max_eng_len = 20\n",
    "max_hin_len = 20\n",
    "\n",
    "encoder_input = pad_sequences(eng_seq, maxlen=max_eng_len, padding='post')\n",
    "decoder_input = pad_sequences(hin_seq, maxlen=max_hin_len, padding='post')\n",
    "\n",
    "# Decoder target: shift decoder_input left by 1\n",
    "decoder_target = np.zeros((decoder_input.shape[0], decoder_input.shape[1], 1), dtype='float32')\n",
    "decoder_target[:, 0:-1, 0] = decoder_input[:, 1:]\n",
    "\n",
    "# Model Architecture\n",
    "latent_dim = 256\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(eng_vocab_size, latent_dim)(encoder_inputs)\n",
    "enc_outputs, state_h, state_c = LSTM(latent_dim, return_state=True)(enc_emb)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(hin_vocab_size, latent_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = Dense(hin_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "model.fit([encoder_input, decoder_input], decoder_target,\n",
    "          batch_size=64, epochs=20, validation_split=0.2)\n",
    "\n",
    "# Inference Models\n",
    "# Encoder inference\n",
    "encoder_model_inf = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder inference\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_inf_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_inf_outputs, state_h_inf, state_c_inf = decoder_lstm(decoder_inf_emb, initial_state=decoder_states_inputs)\n",
    "decoder_inf_outputs = decoder_dense(decoder_inf_outputs)\n",
    "decoder_model_inf = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_inf_outputs, state_h_inf, state_c_inf]\n",
    ")\n",
    "\n",
    "# Reverse mapping\n",
    "reverse_eng = {v: k for k, v in eng_tokenizer.word_index.items()}\n",
    "reverse_hin = {v: k for k, v in hin_tokenizer.word_index.items()}\n",
    "\n",
    "# Translation function\n",
    "def translate(sentence):\n",
    "    sentence = clean_text(sentence)\n",
    "    seq = eng_tokenizer.texts_to_sequences([sentence])\n",
    "    padded = pad_sequences(seq, maxlen=max_eng_len, padding='post')\n",
    "    states = encoder_model_inf.predict(padded)\n",
    "\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = hin_tokenizer.word_index['start_']\n",
    "\n",
    "    decoded = []\n",
    "    for _ in range(max_hin_len):\n",
    "        output_tokens, h, c = decoder_model_inf.predict([target_seq] + states)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = reverse_hin.get(sampled_token_index, '')\n",
    "\n",
    "        if sampled_word == '_end' or sampled_word == '':\n",
    "            break\n",
    "\n",
    "        decoded.append(sampled_word)\n",
    "\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        states = [h, c]\n",
    "\n",
    "    return ' '.join(decoded)\n",
    "\n",
    "# Test\n",
    "print(\"English: And\")\n",
    "print(\"Hindi:\", translate(\"And\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41c8dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
